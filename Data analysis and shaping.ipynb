{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing `contacts.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### are there any differences between all `contacts.csv` dup records?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data is equal across all dups, no updates in any row\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "c_id__row = {}\n",
    "id_diffs  = {}\n",
    "id_dups   = {}\n",
    "key       = \"primary_contact.id\"\n",
    "with open('contacts.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile,delimiter=\";\")\n",
    "    for row in reader:\n",
    "        _row_key = row[key]\n",
    "        _row = c_id__row.get(_row_key,{})\n",
    "        if row!=_row:\n",
    "            _=id_diffs.setdefault(_row_key,0)\n",
    "            id_diffs[_row_key]+=1\n",
    "            c_id__row[_row_key]=row\n",
    "        else:\n",
    "            _=id_dups.setdefault(_row_key,0)\n",
    "            id_dups[_row_key]+=1\n",
    "\n",
    "#unique data for each id\n",
    "id_diffs = {\n",
    "    _id:_diffs\n",
    "    for _id,_diffs in id_diffs.items()\n",
    "    if _diffs>1\n",
    "}            \n",
    "\n",
    "if not id_diffs:\n",
    "    print(\"data is equal across all dups, no updates in any row\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are there any contact info missing in `places.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common primary_contact ids : 163\n",
      "extra primary_contact from contacts: 467\n",
      "missing primary_contact from places: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import csv\n",
    "\n",
    "files = [\n",
    "    'places.csv','contacts.csv'\n",
    "]\n",
    "key       = \"primary_contact.id\"\n",
    "file__ids = {}\n",
    "for f in files:\n",
    "    file__ids[f]=set()\n",
    "    with open(f, encoding='latin-1') as csvfile:\n",
    "        reader = csv.DictReader(csvfile,delimiter=\";\")\n",
    "        for row in reader:\n",
    "            row_key = row[key]\n",
    "            #skip empty values\n",
    "            if row_key.strip():\n",
    "                file__ids[f].add(row_key)\n",
    "\n",
    "\n",
    "places_ids   = file__ids['places.csv']\n",
    "contacts_ids = file__ids['contacts.csv']\n",
    "print (\"common primary_contact ids : {}\".format(\n",
    "        len(contacts_ids.intersection(places_ids))\n",
    "    )\n",
    ")\n",
    "print (\"extra primary_contact from contacts: {}\".format(\n",
    "        len(contacts_ids-places_ids)\n",
    "    )\n",
    ")\n",
    "print (\"missing primary_contact from places: {}\".format(\n",
    "        len(places_ids-contacts_ids)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First conclusion: `contacts.csv` is a *redundant* data file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data shapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare `places.csv` for elastic-kibana insertion. We're gonna create a well-formated data file since in csv numbers and booleans are stored as text as long as comma-separated values are stored as a chunk of text.     \n",
    "In order to allow a good drill-down filtering we must split comma-separated joined-value in lists of values.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enrichment\n",
    "- geopoints should be joined by comma in a single field, instead of having 2 separated values.\n",
    "- In order to build an state-map of inputs, let's standarize the `input_state` field to `input_state_fips` (https://en.wikipedia.org/wiki/Federal_Information_Processing_Standard_state_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import ndjson\n",
    "import json\n",
    "_file   = \"places.csv\"\n",
    "_output = \"data_ingest.ndjson\"\n",
    "\n",
    "\n",
    "def guess_type(s):\n",
    "    try:\n",
    "        value = json.loads(s)\n",
    "    except ValueError:\n",
    "        return str\n",
    "    else:\n",
    "        return type(value)\n",
    "\n",
    "fips_mapping = {\n",
    "    \"california\"   : \"CA\",\n",
    "    \"texas\"        : \"TX\",\n",
    "    \"florida\"      : \"FL\",\n",
    "    \"colorado\"     : \"CO\",\n",
    "    \"washington\"   : \"WA\",\n",
    "    \"ohio\"         : \"OH\",\n",
    "    \"pennsylvania\" : \"PA\",\n",
    "    \"new_york\"     : \"NY\",\n",
    "    \"NULL\"         : None,\n",
    "    \"nevada\"       : \"NV\",\n",
    "    \"arizona\"      : \"AZ\",\n",
    "}\n",
    "geo_points = [\"geocoordinate\",\"manual_geocoordinate\"]\n",
    "split_values = [\n",
    "    \"matched_rules\",#['address', ' phone']\n",
    "    \"sic_code_ids\",#['336501', '354405']\n",
    "    \"labels.sic_code_ids\",#['Foundries-Aluminum Brass Bronze (Mfrs)', 'Molds (Mfrs)']\n",
    "    \"naics_code_ids\",#['33152304', '33351410']\n",
    "    \"labels.naics_code_ids\",#['Foundries-Aluminum Brass Bronze (Mfrs)', 'Molds (Mfrs)']\n",
    "    \"payment_types\",#['Visa', 'MasterCard', 'American Express', 'Discover', 'Check', 'Cash']\n",
    "    \"labels.payment_types\",#['Visa', 'Mastercard', 'American Express', 'Discover', 'Check', 'Cash']\n",
    "    \"record_list_ids\",#['3054', '4902']\n",
    "    \"website_keywords\",#['brushholder castings', 'brushholder rebuild', 'brushholder repair/rebuild', 'brushholders', 'carbon brush', 'carbon brush for commutator', 'commutator brushes', 'constant force springs', 'diamond brushholders', 'diamond carbon', 'electric carbon graphite', 'flat wire springs', 'fulmer company motor brushes', 'leveling wedges', 'locomotive', 'mill motor and mine haulage', 'pantographs', 'perma cast', 'plating lines', 'pressure finger assemblies', 'round wire springs', 'shunts', 'slip ring brushes', 'slip rings', 'specialty machining castings carbon', 'spring assemblies', 'transit']\n",
    "    \"historical_names\",#['Memorial Hermann Surgical Hosp', 'Memorial Hermann Surgical Hospital']\n",
    "    \"business_type_ids\",#['329ff482-23a0-11e2-94ad-a9a9b1ace525', '32a263de-23a0-11e2-9ddf-ffc48ffb142a', '32a6ec2e-23a0-11e2-9745-41f2c6dbe07f']\n",
    "    \"labels.business_type_ids\",#['Emergency Room', 'For Profit', 'General Medical/Surgical']\n",
    "    \"professional_specialty_ids\",#['31fc0700-23a0-11e2-820f-18f582d88ad5', '303d4fa0-23a0-11e2-871e-dfd32e4bbfd5', '30640dd4-23a0-11e2-8f70-ca85a13a8964']\n",
    "    \"labels.professional_specialty_ids\",#['Family Practice', 'Internal Medicine', 'Pulmonary Disease']\n",
    "    \"operating_hours_description\",#['Tue 10am-3pm', ' Fri-Sat 10am-3pm', ' Wed-Thu 1pm-7pm']\n",
    "    \"input_email\",#['sunithab.09@gmail.com', ' sunitha@momgeni.com']\n",
    "    \"input_name\",#['Alex Samples (exec assistant)', ' Lauren Adams (Boss) ']\n",
    "    \"eins\",#['204496500', '943130371']\n",
    "    \"suppressed_fields\",#['street', 'building_number', 'carrier_route_code', 'census_block', 'census_tract', 'delivery_point_bar_code', 'entry_point_match_level_restricted', 'geo_match_level', 'infogroup_match_type_code', 'lat', 'latitude', 'latitude_entry_point_restricted', 'lon', 'longitude', 'longitude_entry_point_restricted', 'suite', 'zip_four', 'mailing_score_code', 'mailing_confidence_code', 'manual_geocoordinate', 'landmark_address']\n",
    "    \"insurances_accepted\",#['aetna', 'dental dppo']\n",
    "    \"primary_contact.title_codes\",#['C', '1', '2']\n",
    "    \"labels.primary_contact.title_codes\",#['Chief Executive Officer', 'Owner', 'President']\n",
    "    \"ancestor_headquarters_ids\",#['408126506', '002532471']\n",
    "    \"languages_spoken\",#['english', 'spanish']\n",
    "    \"labels.languages_spoken\",#['English', 'Spanish']\n",
    "]\n",
    "\n",
    "with open(_file, encoding='latin-1') as csvfile,open(_output, 'w') as f:\n",
    "    reader = csv.DictReader(csvfile,delimiter=\";\")\n",
    "    writer = ndjson.writer(f, ensure_ascii=False)\n",
    "    for row in reader:\n",
    "        item = {}\n",
    "        for k,v in row.items():\n",
    "            if k in split_values:\n",
    "                v = [_v.strip() for _v in v.split(\",\")]\n",
    "            else:\n",
    "                _type = guess_type(v)\n",
    "                v = _type(v)\n",
    "            if v:\n",
    "                item[k]=v\n",
    "\n",
    "        #enrichment data\n",
    "        #geo_points\n",
    "        for k in geo_points:\n",
    "            _lat,_lon = item.pop('{}.lat'.format(k),'') , item.pop('{}.lon'.format(k),'')\n",
    "            if _lat and _lon:\n",
    "                item[k]=\"{},{}\".format(_lat , _lon)\n",
    "\n",
    "        #standarize input states\n",
    "        _input_state = item.get('input_state','')\n",
    "        if _input_state and fips_mapping.get(_input_state,None):\n",
    "            item['input_state_fips'] = fips_mapping.get(_input_state,None)\n",
    "        \n",
    "        writer.writerow(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kibana\n",
    "Kibana helps you import data, however this time it has to be tunned replacing all `text` by `keywords` mappings unless `company_description` field that is free text. `keyword` typed fields allows aggregations, the key feature in this POC.    \n",
    "Also pay attention to location fields `geocoordinate` and `manual_geocoordinate`, they should be mapped as `geo_point` in order to build maps using them.\n",
    "The wizard-created index pattern should be replaced to non-time-basis one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
